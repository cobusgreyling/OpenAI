{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPj5XYvE61j0efeIR3GjqrE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cobusgreyling/OpenAI/blob/main/OpenAI_Token_Counter.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install --upgrade tiktoken"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qmy0BRhXHFsF",
        "outputId": "1ae70739-9bbf-4abb-c57c-0beda3cddcca"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.10/dist-packages (0.5.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2023.6.3)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2023.7.22)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uX0CDzw0G1FJ",
        "outputId": "591e198b-f4ab-4a02-c239-758fefdfd3f0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (1.3.5)\n",
            "Requirement already satisfied: anyio<4,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.25.2)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (1.10.13)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.5 in /usr/local/lib/python3.10/dist-packages (from openai) (4.5.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.5.0->openai) (3.4)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.5.0->openai) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.5.0->openai) (1.1.3)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2023.7.22)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.2)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n"
          ]
        }
      ],
      "source": [
        "%pip install --upgrade openai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tiktoken"
      ],
      "metadata": {
        "id": "3t8bACOQHNMX"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import openai\n"
      ],
      "metadata": {
        "id": "fU60CmfDHlv3"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "openai.api_key = \"your api key goes here\""
      ],
      "metadata": {
        "id": "UqOYFIyMHKn1"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoding = tiktoken.get_encoding(\"cl100k_base\")"
      ],
      "metadata": {
        "id": "UdPsfCtBHs2T"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoding = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")"
      ],
      "metadata": {
        "id": "NjAsoBNnHw_F"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoding.encode(\"How long is the great wall of China?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s8j19sYVINBb",
        "outputId": "d312b97d-fdd6-4333-cc15-e87af012c542"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[4438, 1317, 374, 279, 2294, 7147, 315, 5734, 30]"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def num_tokens_from_string(string: str, encoding_name: str) -> int:\n",
        "    \"\"\"Returns the number of tokens in a text string.\"\"\"\n",
        "    encoding = tiktoken.get_encoding(encoding_name)\n",
        "    num_tokens = len(encoding.encode(string))\n",
        "    return num_tokens"
      ],
      "metadata": {
        "id": "9iT55lw_IQIG"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_tokens_from_string(\"How long is the great wall of China?\", \"cl100k_base\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9xZhUH39ITrd",
        "outputId": "418925e2-bf41-48de-e230-173ffa9a3543"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # # # # # # # # # # # # # # # # # # # # # # # #\n",
        "# Turn tokens into text with encoding.decode()  #\n",
        "# # # # # # # # # # # # # # # # # # # # # # # # #"
      ],
      "metadata": {
        "id": "cfAnd-X6IVfT"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoding.decode([4438, 1317, 374, 279, 2294, 7147, 315, 5734, 30])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "r_i0VsOVJAg_",
        "outputId": "3383f08a-1671-4566-8cf6-edb2b7157219"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'How long is the great wall of China?'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "[encoding.decode_single_token_bytes(token) for token in [84438, 1317, 374, 279, 2294, 7147, 315, 5734, 30]]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8l09i9BuJVrm",
        "outputId": "fe16601d-a6cd-4965-f8fd-abf2734533a2"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[b'_twitter',\n",
              " b' long',\n",
              " b' is',\n",
              " b' the',\n",
              " b' great',\n",
              " b' wall',\n",
              " b' of',\n",
              " b' China',\n",
              " b'?']"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # # # # # # # # # # # # # # # # # # # # # # # #\n",
        "#             Comparing Encodings               #\n",
        "# # # # # # # # # # # # # # # # # # # # # # # # #"
      ],
      "metadata": {
        "id": "8nTp9R97Jged"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compare_encodings(example_string: str) -> None:\n",
        "    \"\"\"Prints a comparison of three string encodings.\"\"\"\n",
        "    # print the example string\n",
        "    print(f'\\nExample string: \"{example_string}\"')\n",
        "    # for each encoding, print the # of tokens, the token integers, and the token bytes\n",
        "    for encoding_name in [\"r50k_base\", \"p50k_base\", \"cl100k_base\"]:\n",
        "        encoding = tiktoken.get_encoding(encoding_name)\n",
        "        token_integers = encoding.encode(example_string)\n",
        "        num_tokens = len(token_integers)\n",
        "        token_bytes = [encoding.decode_single_token_bytes(token) for token in token_integers]\n",
        "        print()\n",
        "        print(f\"{encoding_name}: {num_tokens} tokens\")\n",
        "        print(f\"token integers: {token_integers}\")\n",
        "        print(f\"token bytes: {token_bytes}\")"
      ],
      "metadata": {
        "id": "IFhFo47hJzs0"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "compare_encodings(\"How long is the great wall of China?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JpW1aGLKJ1NK",
        "outputId": "3b9b8a75-975c-4a79-adfb-41579d7b0d63"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Example string: \"How long is the great wall of China?\"\n",
            "\n",
            "r50k_base: 9 tokens\n",
            "token integers: [2437, 890, 318, 262, 1049, 3355, 286, 2807, 30]\n",
            "token bytes: [b'How', b' long', b' is', b' the', b' great', b' wall', b' of', b' China', b'?']\n",
            "\n",
            "p50k_base: 9 tokens\n",
            "token integers: [2437, 890, 318, 262, 1049, 3355, 286, 2807, 30]\n",
            "token bytes: [b'How', b' long', b' is', b' the', b' great', b' wall', b' of', b' China', b'?']\n",
            "\n",
            "cl100k_base: 9 tokens\n",
            "token integers: [4438, 1317, 374, 279, 2294, 7147, 315, 5734, 30]\n",
            "token bytes: [b'How', b' long', b' is', b' the', b' great', b' wall', b' of', b' China', b'?']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "compare_encodings(\"2 + 2 = 4\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HWGFUXymKL0W",
        "outputId": "d95d5511-3006-408d-e979-180eba7e836f"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Example string: \"2 + 2 = 4\"\n",
            "\n",
            "r50k_base: 5 tokens\n",
            "token integers: [17, 1343, 362, 796, 604]\n",
            "token bytes: [b'2', b' +', b' 2', b' =', b' 4']\n",
            "\n",
            "p50k_base: 5 tokens\n",
            "token integers: [17, 1343, 362, 796, 604]\n",
            "token bytes: [b'2', b' +', b' 2', b' =', b' 4']\n",
            "\n",
            "cl100k_base: 7 tokens\n",
            "token integers: [17, 489, 220, 17, 284, 220, 19]\n",
            "token bytes: [b'2', b' +', b' ', b'2', b' =', b' ', b'4']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # # # # # # # # # # # # # # # # # # # # # # #  # #\n",
        "#  Counting tokens for chat completions API calls  #\n",
        "# # # # # # # # # # # # # # # # # # # # # # # ## # #"
      ],
      "metadata": {
        "id": "FVOiMZHbKO9G"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def num_tokens_from_messages(messages, model=\"gpt-3.5-turbo-0613\"):\n",
        "    \"\"\"Return the number of tokens used by a list of messages.\"\"\"\n",
        "    try:\n",
        "        encoding = tiktoken.encoding_for_model(model)\n",
        "    except KeyError:\n",
        "        print(\"Warning: model not found. Using cl100k_base encoding.\")\n",
        "        encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
        "    if model in {\n",
        "        \"gpt-3.5-turbo-0613\",\n",
        "        \"gpt-3.5-turbo-16k-0613\",\n",
        "        \"gpt-4-0314\",\n",
        "        \"gpt-4-32k-0314\",\n",
        "        \"gpt-4-0613\",\n",
        "        \"gpt-4-32k-0613\",\n",
        "        }:\n",
        "        tokens_per_message = 3\n",
        "        tokens_per_name = 1\n",
        "    elif model == \"gpt-3.5-turbo-0301\":\n",
        "        tokens_per_message = 4  # every message follows <|start|>{role/name}\\n{content}<|end|>\\n\n",
        "        tokens_per_name = -1  # if there's a name, the role is omitted\n",
        "    elif \"gpt-3.5-turbo\" in model:\n",
        "        print(\"Warning: gpt-3.5-turbo may update over time. Returning num tokens assuming gpt-3.5-turbo-0613.\")\n",
        "        return num_tokens_from_messages(messages, model=\"gpt-3.5-turbo-0613\")\n",
        "    elif \"gpt-4\" in model:\n",
        "        print(\"Warning: gpt-4 may update over time. Returning num tokens assuming gpt-4-0613.\")\n",
        "        return num_tokens_from_messages(messages, model=\"gpt-4-0613\")\n",
        "    else:\n",
        "        raise NotImplementedError(\n",
        "            f\"\"\"num_tokens_from_messages() is not implemented for model {model}. See https://github.com/openai/openai-python/blob/main/chatml.md for information on how messages are converted to tokens.\"\"\"\n",
        "        )\n",
        "    num_tokens = 0\n",
        "    for message in messages:\n",
        "        num_tokens += tokens_per_message\n",
        "        for key, value in message.items():\n",
        "            num_tokens += len(encoding.encode(value))\n",
        "            if key == \"name\":\n",
        "                num_tokens += tokens_per_name\n",
        "    num_tokens += 3  # every reply is primed with <|start|>assistant<|message|>\n",
        "    return num_tokens"
      ],
      "metadata": {
        "id": "Lq-pMvR8Lz0n"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ['OPENAI_API_KEY'] = str(\"your api key goes here\")\n",
        "from openai import OpenAI\n",
        "client = OpenAI()\n",
        "\n",
        "# let's verify the function above matches the OpenAI API response\n",
        "\n",
        "import openai\n",
        "\n",
        "example_messages = [\n",
        "    {\n",
        "        \"role\": \"system\",\n",
        "        \"content\": \"You are a helpful, pattern-following assistant that translates corporate jargon into plain English.\",\n",
        "    },\n",
        "    {\n",
        "        \"role\": \"system\",\n",
        "        \"name\": \"example_user\",\n",
        "        \"content\": \"New synergies will help drive top-line growth.\",\n",
        "    },\n",
        "    {\n",
        "        \"role\": \"system\",\n",
        "        \"name\": \"example_assistant\",\n",
        "        \"content\": \"Things working well together will increase revenue.\",\n",
        "    },\n",
        "    {\n",
        "        \"role\": \"system\",\n",
        "        \"name\": \"example_user\",\n",
        "        \"content\": \"Let's circle back when we have more bandwidth to touch base on opportunities for increased leverage.\",\n",
        "    },\n",
        "    {\n",
        "        \"role\": \"system\",\n",
        "        \"name\": \"example_assistant\",\n",
        "        \"content\": \"Let's talk later when we're less busy about how to do better.\",\n",
        "    },\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": \"This late pivot means we don't have time to boil the ocean for the client deliverable.\",\n",
        "    },\n",
        "]\n",
        "###########################################################################\n",
        "for model in [\n",
        "    \"gpt-3.5-turbo-0301\",\n",
        "    \"gpt-3.5-turbo-0613\",\n",
        "    \"gpt-3.5-turbo\",\n",
        "    \"gpt-4-0314\",\n",
        "    \"gpt-4-0613\",\n",
        "    \"gpt-4\",\n",
        "    ]:\n",
        "    print(model)\n",
        "    # example token count from the function defined above\n",
        "    print(f\"{num_tokens_from_messages(example_messages, model)} prompt tokens counted by num_tokens_from_messages().\")\n",
        "    # example token count from the OpenAI API\n",
        "\n",
        "\n",
        "    response = client.chat.completions.create(\n",
        "    model=model,\n",
        "    messages=example_messages,\n",
        "    temperature=1,\n",
        "    max_tokens=30,\n",
        "    top_p=1,\n",
        "    frequency_penalty=0,\n",
        "    presence_penalty=0\n",
        "  )\n",
        "    #print(f'{response[\"usage\"][\"prompt_tokens\"]} prompt tokens counted by the OpenAI API.')\n",
        "    #print(response.choices[0].message.content). completion_tokens=28, prompt_tokens=12, total_tokens=40)\n",
        "    print(\"Number of Prompt Tokens Tolens:\", response.usage.prompt_tokens)\n",
        "    print(\"Number of completion Tokens:\", response.usage.completion_tokens)\n",
        "    print(\"Number of Total Tokens:\", response.usage.total_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2eVhm5bTL4VL",
        "outputId": "0a03e80b-bbc0-4911-d36a-d069f7358e39"
      },
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "gpt-3.5-turbo-0301\n",
            "127 prompt tokens counted by num_tokens_from_messages().\n",
            "Number of Prompt Tokens Tolens: 127\n",
            "Number of completion Tokens: 23\n",
            "Number of Total Tokens: 150\n",
            "gpt-3.5-turbo-0613\n",
            "129 prompt tokens counted by num_tokens_from_messages().\n",
            "Number of Prompt Tokens Tolens: 129\n",
            "Number of completion Tokens: 20\n",
            "Number of Total Tokens: 149\n",
            "gpt-3.5-turbo\n",
            "Warning: gpt-3.5-turbo may update over time. Returning num tokens assuming gpt-3.5-turbo-0613.\n",
            "129 prompt tokens counted by num_tokens_from_messages().\n",
            "Number of Prompt Tokens Tolens: 129\n",
            "Number of completion Tokens: 21\n",
            "Number of Total Tokens: 150\n",
            "gpt-4-0314\n",
            "129 prompt tokens counted by num_tokens_from_messages().\n",
            "Number of Prompt Tokens Tolens: 129\n",
            "Number of completion Tokens: 17\n",
            "Number of Total Tokens: 146\n",
            "gpt-4-0613\n",
            "129 prompt tokens counted by num_tokens_from_messages().\n",
            "Number of Prompt Tokens Tolens: 129\n",
            "Number of completion Tokens: 18\n",
            "Number of Total Tokens: 147\n",
            "gpt-4\n",
            "Warning: gpt-4 may update over time. Returning num tokens assuming gpt-4-0613.\n",
            "129 prompt tokens counted by num_tokens_from_messages().\n",
            "Number of Prompt Tokens Tolens: 129\n",
            "Number of completion Tokens: 19\n",
            "Number of Total Tokens: 148\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ['OPENAI_API_KEY'] = str(\"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\")\n",
        "from openai import OpenAI\n",
        "client = OpenAI()\n",
        "\n",
        "# This code is for v1 of the openai package: pypi.org/project/openai\n",
        "from openai import OpenAI\n",
        "client = OpenAI()\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "  model=\"gpt-3.5-turbo\",\n",
        "  messages=[\n",
        "    {\n",
        "      \"role\": \"user\",\n",
        "      \"content\": \"what is H20?\"\n",
        "    }\n",
        "  ],\n",
        "  temperature=1,\n",
        "  max_tokens=256,\n",
        "  top_p=1,\n",
        "  frequency_penalty=0,\n",
        "  presence_penalty=0\n",
        ")\n",
        "\n",
        "print (response)\n",
        "#print(response.choices[0].completion_tokens)\n",
        "print(response.usage.completion_tokens)\n",
        "print(response.usage.prompt_tokens)\n",
        "print(response.usage.total_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jo_TDgU6UELo",
        "outputId": "c811fdca-44c1-42c1-f717-892f92173eb6"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ChatCompletion(id='chatcmpl-8Pl4ADmvY0DOjeea9t7LcfNJfLDmr', choices=[Choice(finish_reason='stop', index=0, message=ChatCompletionMessage(content='H20 is the chemical formula for water. It represents one molecule of water, which consists of two hydrogen atoms bonded to one oxygen atom.', role='assistant', function_call=None, tool_calls=None))], created=1701149814, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=28, prompt_tokens=12, total_tokens=40))\n",
            "28\n",
            "12\n",
            "40\n"
          ]
        }
      ]
    }
  ]
}