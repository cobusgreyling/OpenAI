{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOrRQIxQTaiXmvsTY/HIRqg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cobusgreyling/OpenAI/blob/main/OpenAI_Token_Counter.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install --upgrade tiktoken"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qmy0BRhXHFsF",
        "outputId": "b9909005-3d69-427c-d9fc-33271178757d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.5.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/2.0 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.1/2.0 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━\u001b[0m \u001b[32m1.4/2.0 MB\u001b[0m \u001b[31m21.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m22.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2023.6.3)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2023.7.22)\n",
            "Installing collected packages: tiktoken\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\n",
            "llmx 0.0.15a0 requires openai, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed tiktoken-0.5.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uX0CDzw0G1FJ",
        "outputId": "19a1221b-2236-4e4e-928f-0eeca18a3015"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai\n",
            "  Downloading openai-1.3.5-py3-none-any.whl (220 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/220.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━\u001b[0m \u001b[32m215.0/220.8 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m220.8/220.8 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: anyio<4,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai)\n",
            "  Downloading httpx-0.25.2-py3-none-any.whl (74 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.0/75.0 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (1.10.13)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.5 in /usr/local/lib/python3.10/dist-packages (from openai) (4.5.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.5.0->openai) (3.4)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.5.0->openai) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.5.0->openai) (1.1.3)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2023.7.22)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n",
            "  Downloading httpcore-1.0.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.9/76.9 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: h11, httpcore, httpx, openai\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed h11-0.14.0 httpcore-1.0.2 httpx-0.25.2 openai-1.3.5\n"
          ]
        }
      ],
      "source": [
        "%pip install --upgrade openai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tiktoken"
      ],
      "metadata": {
        "id": "3t8bACOQHNMX"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import openai\n"
      ],
      "metadata": {
        "id": "fU60CmfDHlv3"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "openai.api_key = \"Your api key goes here\""
      ],
      "metadata": {
        "id": "UqOYFIyMHKn1"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoding = tiktoken.get_encoding(\"cl100k_base\")"
      ],
      "metadata": {
        "id": "UdPsfCtBHs2T"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoding = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")"
      ],
      "metadata": {
        "id": "NjAsoBNnHw_F"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoding.encode(\"How long is the great wall of China?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s8j19sYVINBb",
        "outputId": "f8582883-7fd9-49c1-d92c-9762516f4c21"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[4438, 1317, 374, 279, 2294, 7147, 315, 5734, 30]"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def num_tokens_from_string(string: str, encoding_name: str) -> int:\n",
        "    \"\"\"Returns the number of tokens in a text string.\"\"\"\n",
        "    encoding = tiktoken.get_encoding(encoding_name)\n",
        "    num_tokens = len(encoding.encode(string))\n",
        "    return num_tokens"
      ],
      "metadata": {
        "id": "9iT55lw_IQIG"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_tokens_from_string(\"How long is the great wall of China?\", \"cl100k_base\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9xZhUH39ITrd",
        "outputId": "26153fdc-3cfe-4f9b-e34b-edb78a6bc120"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # # # # # # # # # # # # # # # # # # # # # # # #\n",
        "# Turn tokens into text with encoding.decode()  #\n",
        "# # # # # # # # # # # # # # # # # # # # # # # # #"
      ],
      "metadata": {
        "id": "cfAnd-X6IVfT"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoding.decode([4438, 1317, 374, 279, 2294, 7147, 315, 5734, 30])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "r_i0VsOVJAg_",
        "outputId": "fdd6c7e7-5448-4652-c3b8-6e1f9a6977dd"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'How long is the great wall of China?'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "[encoding.decode_single_token_bytes(token) for token in [4438, 1317, 374, 279, 2294, 7147, 315, 5734, 30]]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8l09i9BuJVrm",
        "outputId": "fd2cf098-fdbd-48d5-8717-530e7d04a36a"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[b'How',\n",
              " b' long',\n",
              " b' is',\n",
              " b' the',\n",
              " b' great',\n",
              " b' wall',\n",
              " b' of',\n",
              " b' China',\n",
              " b'?']"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # # # # # # # # # # # # # # # # # # # # # # # #\n",
        "#             Comparing Encodings               #\n",
        "# # # # # # # # # # # # # # # # # # # # # # # # #"
      ],
      "metadata": {
        "id": "8nTp9R97Jged"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compare_encodings(example_string: str) -> None:\n",
        "    \"\"\"Prints a comparison of three string encodings.\"\"\"\n",
        "    # print the example string\n",
        "    print(f'\\nExample string: \"{example_string}\"')\n",
        "    # for each encoding, print the # of tokens, the token integers, and the token bytes\n",
        "    for encoding_name in [\"r50k_base\", \"p50k_base\", \"cl100k_base\"]:\n",
        "        encoding = tiktoken.get_encoding(encoding_name)\n",
        "        token_integers = encoding.encode(example_string)\n",
        "        num_tokens = len(token_integers)\n",
        "        token_bytes = [encoding.decode_single_token_bytes(token) for token in token_integers]\n",
        "        print()\n",
        "        print(f\"{encoding_name}: {num_tokens} tokens\")\n",
        "        print(f\"token integers: {token_integers}\")\n",
        "        print(f\"token bytes: {token_bytes}\")"
      ],
      "metadata": {
        "id": "IFhFo47hJzs0"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "compare_encodings(\"How long is the great wall of China?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JpW1aGLKJ1NK",
        "outputId": "3b9b8a75-975c-4a79-adfb-41579d7b0d63"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Example string: \"How long is the great wall of China?\"\n",
            "\n",
            "r50k_base: 9 tokens\n",
            "token integers: [2437, 890, 318, 262, 1049, 3355, 286, 2807, 30]\n",
            "token bytes: [b'How', b' long', b' is', b' the', b' great', b' wall', b' of', b' China', b'?']\n",
            "\n",
            "p50k_base: 9 tokens\n",
            "token integers: [2437, 890, 318, 262, 1049, 3355, 286, 2807, 30]\n",
            "token bytes: [b'How', b' long', b' is', b' the', b' great', b' wall', b' of', b' China', b'?']\n",
            "\n",
            "cl100k_base: 9 tokens\n",
            "token integers: [4438, 1317, 374, 279, 2294, 7147, 315, 5734, 30]\n",
            "token bytes: [b'How', b' long', b' is', b' the', b' great', b' wall', b' of', b' China', b'?']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "compare_encodings(\"2 + 2 = 4\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HWGFUXymKL0W",
        "outputId": "d95d5511-3006-408d-e979-180eba7e836f"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Example string: \"2 + 2 = 4\"\n",
            "\n",
            "r50k_base: 5 tokens\n",
            "token integers: [17, 1343, 362, 796, 604]\n",
            "token bytes: [b'2', b' +', b' 2', b' =', b' 4']\n",
            "\n",
            "p50k_base: 5 tokens\n",
            "token integers: [17, 1343, 362, 796, 604]\n",
            "token bytes: [b'2', b' +', b' 2', b' =', b' 4']\n",
            "\n",
            "cl100k_base: 7 tokens\n",
            "token integers: [17, 489, 220, 17, 284, 220, 19]\n",
            "token bytes: [b'2', b' +', b' ', b'2', b' =', b' ', b'4']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # # # # # # # # # # # # # # # # # # # # # # #  # #\n",
        "#  Counting tokens for chat completions API calls  #\n",
        "# # # # # # # # # # # # # # # # # # # # # # # ## # #"
      ],
      "metadata": {
        "id": "FVOiMZHbKO9G"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def num_tokens_from_messages(messages, model=\"gpt-3.5-turbo-0613\"):\n",
        "    \"\"\"Return the number of tokens used by a list of messages.\"\"\"\n",
        "    try:\n",
        "        encoding = tiktoken.encoding_for_model(model)\n",
        "    except KeyError:\n",
        "        print(\"Warning: model not found. Using cl100k_base encoding.\")\n",
        "        encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
        "    if model in {\n",
        "        \"gpt-3.5-turbo-0613\",\n",
        "        \"gpt-3.5-turbo-16k-0613\",\n",
        "        \"gpt-4-0314\",\n",
        "        \"gpt-4-32k-0314\",\n",
        "        \"gpt-4-0613\",\n",
        "        \"gpt-4-32k-0613\",\n",
        "        }:\n",
        "        tokens_per_message = 3\n",
        "        tokens_per_name = 1\n",
        "    elif model == \"gpt-3.5-turbo-0301\":\n",
        "        tokens_per_message = 4  # every message follows <|start|>{role/name}\\n{content}<|end|>\\n\n",
        "        tokens_per_name = -1  # if there's a name, the role is omitted\n",
        "    elif \"gpt-3.5-turbo\" in model:\n",
        "        print(\"Warning: gpt-3.5-turbo may update over time. Returning num tokens assuming gpt-3.5-turbo-0613.\")\n",
        "        return num_tokens_from_messages(messages, model=\"gpt-3.5-turbo-0613\")\n",
        "    elif \"gpt-4\" in model:\n",
        "        print(\"Warning: gpt-4 may update over time. Returning num tokens assuming gpt-4-0613.\")\n",
        "        return num_tokens_from_messages(messages, model=\"gpt-4-0613\")\n",
        "    else:\n",
        "        raise NotImplementedError(\n",
        "            f\"\"\"num_tokens_from_messages() is not implemented for model {model}. See https://github.com/openai/openai-python/blob/main/chatml.md for information on how messages are converted to tokens.\"\"\"\n",
        "        )\n",
        "    num_tokens = 0\n",
        "    for message in messages:\n",
        "        num_tokens += tokens_per_message\n",
        "        for key, value in message.items():\n",
        "            num_tokens += len(encoding.encode(value))\n",
        "            if key == \"name\":\n",
        "                num_tokens += tokens_per_name\n",
        "    num_tokens += 3  # every reply is primed with <|start|>assistant<|message|>\n",
        "    return num_tokens"
      ],
      "metadata": {
        "id": "Lq-pMvR8Lz0n"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ['OPENAI_API_KEY'] = str(\"your api key goes here\")\n",
        "from openai import OpenAI\n",
        "client = OpenAI()\n",
        "\n",
        "# let's verify the function above matches the OpenAI API response\n",
        "\n",
        "import openai\n",
        "\n",
        "example_messages = [\n",
        "    {\n",
        "        \"role\": \"system\",\n",
        "        \"content\": \"You are a helpful, pattern-following assistant that translates corporate jargon into plain English.\",\n",
        "    },\n",
        "    {\n",
        "        \"role\": \"system\",\n",
        "        \"name\": \"example_user\",\n",
        "        \"content\": \"New synergies will help drive top-line growth.\",\n",
        "    },\n",
        "    {\n",
        "        \"role\": \"system\",\n",
        "        \"name\": \"example_assistant\",\n",
        "        \"content\": \"Things working well together will increase revenue.\",\n",
        "    },\n",
        "    {\n",
        "        \"role\": \"system\",\n",
        "        \"name\": \"example_user\",\n",
        "        \"content\": \"Let's circle back when we have more bandwidth to touch base on opportunities for increased leverage.\",\n",
        "    },\n",
        "    {\n",
        "        \"role\": \"system\",\n",
        "        \"name\": \"example_assistant\",\n",
        "        \"content\": \"Let's talk later when we're less busy about how to do better.\",\n",
        "    },\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": \"This late pivot means we don't have time to boil the ocean for the client deliverable.\",\n",
        "    },\n",
        "]\n",
        "###########################################################################\n",
        "for model in [\n",
        "    \"gpt-3.5-turbo-0301\",\n",
        "    \"gpt-3.5-turbo-0613\",\n",
        "    \"gpt-3.5-turbo\",\n",
        "    \"gpt-4-0314\",\n",
        "    \"gpt-4-0613\",\n",
        "    \"gpt-4\",\n",
        "    ]:\n",
        "    print(model)\n",
        "    # example token count from the function defined above\n",
        "    print(f\"{num_tokens_from_messages(example_messages, model)} prompt tokens counted by num_tokens_from_messages().\")\n",
        "    # example token count from the OpenAI API\n",
        "\n",
        "\n",
        "    response = client.chat.completions.create(\n",
        "    model=model,\n",
        "    messages=example_messages,\n",
        "    temperature=1,\n",
        "    max_tokens=30,\n",
        "    top_p=1,\n",
        "    frequency_penalty=0,\n",
        "    presence_penalty=0\n",
        "  )\n",
        "    #print(f'{response[\"usage\"][\"prompt_tokens\"]} prompt tokens counted by the OpenAI API.')\n",
        "    #print(response.choices[0].message.content). completion_tokens=28, prompt_tokens=12, total_tokens=40)\n",
        "    print(\"Number of Prompt Tokens Tolens:\", response.usage.prompt_tokens)\n",
        "    print(\"Number of completion Tokens:\", response.usage.completion_tokens)\n",
        "    print(\"Number of Total Tokens:\", response.usage.total_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2eVhm5bTL4VL",
        "outputId": "0a03e80b-bbc0-4911-d36a-d069f7358e39"
      },
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "gpt-3.5-turbo-0301\n",
            "127 prompt tokens counted by num_tokens_from_messages().\n",
            "Number of Prompt Tokens Tolens: 127\n",
            "Number of completion Tokens: 23\n",
            "Number of Total Tokens: 150\n",
            "gpt-3.5-turbo-0613\n",
            "129 prompt tokens counted by num_tokens_from_messages().\n",
            "Number of Prompt Tokens Tolens: 129\n",
            "Number of completion Tokens: 20\n",
            "Number of Total Tokens: 149\n",
            "gpt-3.5-turbo\n",
            "Warning: gpt-3.5-turbo may update over time. Returning num tokens assuming gpt-3.5-turbo-0613.\n",
            "129 prompt tokens counted by num_tokens_from_messages().\n",
            "Number of Prompt Tokens Tolens: 129\n",
            "Number of completion Tokens: 21\n",
            "Number of Total Tokens: 150\n",
            "gpt-4-0314\n",
            "129 prompt tokens counted by num_tokens_from_messages().\n",
            "Number of Prompt Tokens Tolens: 129\n",
            "Number of completion Tokens: 17\n",
            "Number of Total Tokens: 146\n",
            "gpt-4-0613\n",
            "129 prompt tokens counted by num_tokens_from_messages().\n",
            "Number of Prompt Tokens Tolens: 129\n",
            "Number of completion Tokens: 18\n",
            "Number of Total Tokens: 147\n",
            "gpt-4\n",
            "Warning: gpt-4 may update over time. Returning num tokens assuming gpt-4-0613.\n",
            "129 prompt tokens counted by num_tokens_from_messages().\n",
            "Number of Prompt Tokens Tolens: 129\n",
            "Number of completion Tokens: 19\n",
            "Number of Total Tokens: 148\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ['OPENAI_API_KEY'] = str(\"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\")\n",
        "from openai import OpenAI\n",
        "client = OpenAI()\n",
        "\n",
        "# This code is for v1 of the openai package: pypi.org/project/openai\n",
        "from openai import OpenAI\n",
        "client = OpenAI()\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "  model=\"gpt-3.5-turbo\",\n",
        "  messages=[\n",
        "    {\n",
        "      \"role\": \"user\",\n",
        "      \"content\": \"what is H20?\"\n",
        "    }\n",
        "  ],\n",
        "  temperature=1,\n",
        "  max_tokens=256,\n",
        "  top_p=1,\n",
        "  frequency_penalty=0,\n",
        "  presence_penalty=0\n",
        ")\n",
        "\n",
        "print (response)\n",
        "#print(response.choices[0].completion_tokens)\n",
        "print(response.usage.completion_tokens)\n",
        "print(response.usage.prompt_tokens)\n",
        "print(response.usage.total_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jo_TDgU6UELo",
        "outputId": "c811fdca-44c1-42c1-f717-892f92173eb6"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ChatCompletion(id='chatcmpl-8Pl4ADmvY0DOjeea9t7LcfNJfLDmr', choices=[Choice(finish_reason='stop', index=0, message=ChatCompletionMessage(content='H20 is the chemical formula for water. It represents one molecule of water, which consists of two hydrogen atoms bonded to one oxygen atom.', role='assistant', function_call=None, tool_calls=None))], created=1701149814, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=28, prompt_tokens=12, total_tokens=40))\n",
            "28\n",
            "12\n",
            "40\n"
          ]
        }
      ]
    }
  ]
}